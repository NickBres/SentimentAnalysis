{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:40:19.877243Z",
     "start_time": "2024-02-29T15:40:19.252349Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nickbres/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import nltk  #natural language processing\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('Twitter_Data.csv') # Read the data only with the columns that we need\n",
    "df.head() # Show the first 5 rows of the data\n",
    "df2 = pd.read_csv('Reddit_Data.csv')\n",
    "df2 = df2.rename(columns={'clean_comment': 'clean_text'})\n",
    "df = pd.concat([df, df2])\n",
    "df.reset_index(drop=True, inplace=True) # reset the index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:40:20.276957Z",
     "start_time": "2024-02-29T15:40:19.878530Z"
    }
   },
   "id": "c4a393b71f609f55",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) # Drop the missing values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:40:20.304486Z",
     "start_time": "2024-02-29T15:40:20.278621Z"
    }
   },
   "id": "58719bfb7e9f77cf",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['category'] = df['category'].apply(lambda x: 0 if x == -1 else (1 if x == 0 else 2)) # Convert the labels to numerical values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:40:20.346895Z",
     "start_time": "2024-02-29T15:40:20.306390Z"
    }
   },
   "id": "a57b17ab8ded0666",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original tweet -> when modi promised “minimum government maximum governance” expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples\n",
      "\n",
      "Processed tweet -> ['modi', 'promis', 'minimum', 'govern', 'maximum', 'govern', 'expect', 'begin', 'difficult', 'job', 'reform', 'state', 'take', 'year', 'get', 'justic', 'state', 'busi', 'exit', 'psu', 'templ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def post_to_words(post):\n",
    "    ''' Convert tweet text into a sequence of words '''\n",
    "    # convert to lower case\n",
    "    text = post.lower()\n",
    "    # remove non letters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    # tokenize\n",
    "    words = text.split()\n",
    "    # remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    # apply stemming\n",
    "    words = [PorterStemmer().stem(w) for w in words]\n",
    "    # return list\n",
    "    return words\n",
    "\n",
    "print(\"\\nOriginal tweet ->\", df['clean_text'][0])\n",
    "print(\"\\nProcessed tweet ->\", post_to_words(df['clean_text'][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:40:20.355769Z",
     "start_time": "2024-02-29T15:40:20.347620Z"
    }
   },
   "id": "20f17c46b5d29b3d",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5718ca2dfc739130"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y = df['category'] # Target\n",
    "X = [' '.join(post_to_words(text)) for text in df['clean_text']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:56:51.255547Z",
     "start_time": "2024-02-29T15:53:35.504341Z"
    }
   },
   "id": "f7d3f5cd81f1a1bf",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:57:00.935624Z",
     "start_time": "2024-02-29T15:56:59.456981Z"
    }
   },
   "id": "dc60ba194fbb69eb",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=1000)",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)  # Increasing max_iter for convergence\n",
    "model.fit(X_train_tfidf, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:57:19.669373Z",
     "start_time": "2024-02-29T15:57:01.834148Z"
    }
   },
   "id": "84ca9be5cd3aa280",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.21%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77      4288\n",
      "           1       0.83      0.92      0.87      6847\n",
      "           2       0.89      0.86      0.87      8877\n",
      "\n",
      "    accuracy                           0.85     20012\n",
      "   macro avg       0.85      0.84      0.84     20012\n",
      "weighted avg       0.85      0.85      0.85     20012\n"
     ]
    }
   ],
   "source": [
    "# 3. Prediction and Evaluation\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Detailed performance report\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T15:57:22.166241Z",
     "start_time": "2024-02-29T15:57:22.131168Z"
    }
   },
   "id": "207d63465472cbb2",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text):\n",
    "    text = text.lower() # Convert the text to lowercase\n",
    "    # Convert the text to a pandas DataFrame\n",
    "    df = pd.DataFrame([text], columns=['Tweet'])\n",
    "    \n",
    "    # Preprocess the text\n",
    "    df['Tweet'] = df['Tweet'].apply(post_to_words)\n",
    "    df['Tweet'] = df['Tweet'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    ds = vectorizer.transform(df['Tweet']) # Vectorize the text\n",
    "    \n",
    "    # Use the model to predict the sentiment\n",
    "    prediction = model.predict(ds)\n",
    "\n",
    "    # Since Logistic Regression directly returns the class, no need for argmax\n",
    "    predicted_label = prediction[0]\n",
    "\n",
    "    # Map the numerical label back to the original label\n",
    "    sentiment_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "    predicted_sentiment = sentiment_map[predicted_label]\n",
    "\n",
    "    # Print the predicted sentiment\n",
    "    print(f\"{text} -> {df['Tweet'][0]} -> {predicted_sentiment}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:01:50.462378Z",
     "start_time": "2024-02-29T16:01:50.458065Z"
    }
   },
   "id": "77a2b12472c45930",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am nikita breslavsky -> nikita breslavski -> Neutral\n",
      "i hate nikita breslavsky -> hate nikita breslavski -> Negative\n",
      "i love nikita breslavsky -> love nikita breslavski -> Positive\n",
      "i love to kill people -> love kill peopl -> Positive\n",
      "i love to hate myself -> love hate -> Negative\n",
      "i hate to love myself -> hate love -> Negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predict_sentiment(model, \"I am Nikita Breslavsky\")\n",
    "predict_sentiment(model, \"I hate Nikita Breslavsky\")\n",
    "predict_sentiment(model, \"I love Nikita Breslavsky\")\n",
    "predict_sentiment(model, \"I love to kill people\")\n",
    "predict_sentiment(model, \"I love to hate myself\")\n",
    "predict_sentiment(model, \"I hate to love myself\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:03:11.860350Z",
     "start_time": "2024-02-29T16:03:11.832990Z"
    }
   },
   "id": "8c94d4594f49d73a",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2af064932606c311",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
